{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YM7vlRbCh9pw"
   },
   "source": [
    "# K-Means clustering digits\n",
    "\n",
    "## Pre-reading\n",
    "\n",
    "- [Scikit-learn Clustering](https://scikit-learn.org/stable/modules/clustering.html) sections `2.3.1` and `2.3.2`\n",
    "\n",
    "![K-Means Clustering](https://imgs.xkcd.com/comics/k_means_clustering.png)\n",
    "\n",
    "*According to my especially unsupervised K-means clustering algorithm, there are currently about 8 billion types of people in the world.*\n",
    "\n",
    "\n",
    "### Goals\n",
    "\n",
    "- Learn to import and load datasets with numpy\n",
    "- Use [KMeans](https://scikit-learn.org/stable/modules/clustering.html#k-means) to conduct unsupervised learning\n",
    "- Explore the impact of initialization and iterations\n",
    "- Explore a mistmatch of `K` and the number of classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8nTVH-dh9pz"
   },
   "source": [
    "This lab is modified from the [Scikit Learn example](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html).\n",
    "\n",
    "## Load the dataset\n",
    "\n",
    "We will start by loading the `digits` dataset. This dataset contains\n",
    "handwritten digits from 0 to 9. In the context of clustering, one would like\n",
    "to group images such that the handwritten digits on the image are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnKiXOI4h9pu"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3efUU8UMh9p0",
    "outputId": "4296187c-9533-4671-8247-5cd80475888f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Load the dataset\n",
    "data, labels = load_digits(return_X_y=True)\n",
    "(n_samples, n_features), n_digits = data.shape, np.unique(labels).size\n",
    "\n",
    "print(f\"# digits: {n_digits}; # samples: {n_samples}; # features {n_features}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-ySHdwT8k2ud"
   },
   "source": [
    "We can also display a sample from the dataset, so we know what we are working with.\n",
    "Note that the images are fairly blurry because they are only 8x8 pixels. *Is this enough for our machine learning to reliably work?*\n",
    "\n",
    "Run this a few times to see multiple samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "6v_ZUiJBkxeH",
    "outputId": "1fd00ef3-3a2a-45bf-9952-efcbdb8bb400"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from random import randrange\n",
    "\n",
    "# select a random sample\n",
    "sample_id = randrange(n_samples)\n",
    "sample_image = data[sample_id]\n",
    "\n",
    "# reshape the vector back to a 2D image\n",
    "sample_image = np.reshape(sample_image, (8, 8))\n",
    "\n",
    "# plot the sample\n",
    "print(f\"Sample labeled as {labels[sample_id]}\")\n",
    "plt.imshow(sample_image, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uLVRzcEjh9p6"
   },
   "source": [
    "## Visualize the results on PCA-reduced data\n",
    "\n",
    "[sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "allows to project the data from the original 64-dimensional space into a lower dimensional space.\n",
    "Subsequently, we can use PCA to project into a 2-dimensional space and plot the data and the clusters in this new space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# reduce the 64-dimension data to 2-D\n",
    "pca_data = PCA(n_components=2).fit_transform(data)\n",
    "# simple, non-fancy plot\n",
    "plt.plot(pca_data[:, 0], pca_data[:, 1], \"k.\", markersize=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "w4z4EJH7qC4z"
   },
   "source": [
    "## Iterate with K-Means\n",
    "\n",
    "First, we'll define a method `plot_kmeans` that will add color, centroids, and lines to our PCA plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKMOFnSxh9p6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def plot_kmeans(reduced_data, kmeans, iteration):\n",
    "    # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "    h = 0.02  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Obtain labels for each point in mesh. Use last trained model.\n",
    "    Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Clear previous plot\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    plt.imshow(\n",
    "        Z,\n",
    "        interpolation=\"nearest\",\n",
    "        extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "        cmap=plt.cm.Paired,\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "    )\n",
    "\n",
    "    plt.plot(reduced_data[:, 0], reduced_data[:, 1], \"k.\", markersize=2)\n",
    "    # Plot the centroids as a white X\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    plt.scatter(\n",
    "        centroids[:, 0],\n",
    "        centroids[:, 1],\n",
    "        marker=\"x\",\n",
    "        s=169,\n",
    "        linewidths=3,\n",
    "        color=\"w\",\n",
    "        zorder=10,\n",
    "    )\n",
    "    plt.title(\n",
    "        \"K-means clustering on the digits dataset (PCA-reduced data)\\n\"\n",
    "        \"Centroids are marked with white cross\\n\"\n",
    "        \"Iteration: {}\".format(iteration)\n",
    "    )\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step through kmeans\n",
    "\n",
    "Before we use the builtin fit method, let's step through what the algorithm is doing. We will do this with [MiniBatchKMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html).\n",
    "\n",
    "This first code block randomly picks centroids and then runs k-means once before plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "o6xUdM5Uqf2-",
    "outputId": "bc9a0d05-2666-4162-bce5-0c8eb9d75e1d"
   },
   "outputs": [],
   "source": [
    "max_iterations = 16\n",
    "step_size = 1\n",
    "for i in range(0, max_iterations, step_size):\n",
    "    # step_size iterations between plots\n",
    "    for k in range(step_size):\n",
    "        kmeans_step.partial_fit(reduced_data)\n",
    "\n",
    "    plot_kmeans(reduced_data, kmeans_step, i)\n",
    "    sleep(0.05)\n",
    "\n",
    "plot_kmeans(reduced_data, kmeans_step, max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "# PCA reduction to 2D\n",
    "reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "\n",
    "# initialize\n",
    "kmeans_step = MiniBatchKMeans(\n",
    "    n_clusters=10, init=\"random\", n_init=1, batch_size=10, max_iter=1, random_state=16\n",
    ")\n",
    "\n",
    "kmeans_step.partial_fit(reduced_data)\n",
    "\n",
    "# dispaly clusters\n",
    "plot_kmeans(reduced_data, kmeans_step, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "U9D-rcCtz-CV"
   },
   "source": [
    "## Fit\n",
    "\n",
    "We'll now use the builtin [fit](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.fit) method. This is more realistic for an actual application.\n",
    "\n",
    "Instead of specifying the number of iterations, this method will automatically stop when it **converges**. This is defined as there being no meaningful change of the centroid locations between rounds."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "yVlkqPNwh9p1"
   },
   "source": [
    "### Define our evaluation benchmark\n",
    "\n",
    "We will first our evaluation benchmark. During this benchmark, we intend to\n",
    "compare different initialization methods for KMeans. Our benchmark will:\n",
    "\n",
    "* create a pipeline which will scale the data using a [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "* train and time the pipeline fitting;\n",
    "* measure the performance of the clustering obtained via different metrics.\n",
    "\n",
    "\n",
    "#### Metrics\n",
    "\n",
    "We will pull several metrics from [sklearn.metrics](https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics).\n",
    "It takes a fair amount of knowledge to understand what a set of metrics are communicating. It is worth reading about the ones we are using here.\n",
    "\n",
    "We will use multiple metrics because they communicate different things about our model. Depending on the application, we will likely bias towards\n",
    "accepting one type of error over another. This is based on our assessment of risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MR6nddjbh9p2"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def bench_k_means(kmeans, name, data, labels):\n",
    "    \"\"\"Benchmark to evaluate the KMeans initialization methods.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    kmeans : KMeans instance\n",
    "        A :class:`~sklearn.cluster.KMeans` instance with the initialization\n",
    "        already set.\n",
    "    name : str\n",
    "        Name given to the strategy. It will be used to show the results in a\n",
    "        table.\n",
    "    data : ndarray of shape (n_samples, n_features)\n",
    "        The data to cluster.\n",
    "    labels : ndarray of shape (n_samples,)\n",
    "        The labels used to compute the clustering metrics which requires some\n",
    "        supervision.\n",
    "    \"\"\"\n",
    "    t0 = time()\n",
    "    estimator = make_pipeline(StandardScaler(), kmeans).fit(data)\n",
    "    fit_time = time() - t0\n",
    "    results = [name, fit_time, estimator[-1].inertia_]\n",
    "\n",
    "    # Define the metrics which require only the true labels and estimator labels\n",
    "    clustering_metrics = [\n",
    "        metrics.homogeneity_score,\n",
    "        metrics.completeness_score,\n",
    "        metrics.v_measure_score,\n",
    "    ]\n",
    "\n",
    "    results += [m(labels, estimator[-1].labels_) for m in clustering_metrics]\n",
    "\n",
    "    # The silhouette score requires the full dataset\n",
    "    results += [\n",
    "        metrics.silhouette_score(\n",
    "            data,\n",
    "            estimator[-1].labels_,\n",
    "            metric=\"euclidean\",\n",
    "            sample_size=300,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Show the results\n",
    "    formatter_result = (\n",
    "        # \"{:9s}\\t{:.3f}s\\t{:.0f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\"\n",
    "        \"{:9s}\\t{:.3f}s\\t{:.0f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\"\n",
    "    )\n",
    "    print(formatter_result.format(*results))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qNB_8-Leh9p4"
   },
   "source": [
    "### Run the benchmark\n",
    "\n",
    "We will compare three approaches:\n",
    "\n",
    "* an initialization using `k-means++`. This method is stochastic and we will run the initialization 4 times;\n",
    "* a random initialization. This method is stochastic as well and we will run the initialization 4 times;\n",
    "* an initialization based on a PCA. This method is deterministic and a single initialization suffice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-6X_gVFh9p5",
    "outputId": "ec881c73-7726-42a9-8564-89e560eb03b7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load a fresh copy of the dataset\n",
    "data, labels = load_digits(return_X_y=True)\n",
    "(n_samples, n_features), n_digits = data.shape, np.unique(labels).size\n",
    "print(f\"# digits: {n_digits}; # samples: {n_samples}; # features {n_features}\")\n",
    "\n",
    "# How many clusters\n",
    "n_digits = 10\n",
    "\n",
    "# top bar of stats\n",
    "print(\"Results for\", n_digits, \"clusters\")\n",
    "print(82 * \"_\")\n",
    "print(\"init\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tsilhouette\")\n",
    "\n",
    "# try with k-means++\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=n_digits, n_init=4, random_state=0)\n",
    "bench_k_means(kmeans=kmeans, name=\"k-means++\", data=data, labels=labels)\n",
    "\n",
    "# try with random\n",
    "kmeans = KMeans(init=\"random\", n_clusters=n_digits, n_init=4, random_state=0)\n",
    "bench_k_means(kmeans=kmeans, name=\"random\", data=data, labels=labels)\n",
    "\n",
    "# try with PCA\n",
    "pca = PCA(n_components=n_digits).fit(data)\n",
    "kmeans = KMeans(init=pca.components_, n_clusters=n_digits, n_init=1)\n",
    "bench_k_means(kmeans=kmeans, name=\"PCA-based\", data=data, labels=labels)\n",
    "\n",
    "# bottom bar of stats\n",
    "print(82 * \"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall metrics\n",
    "\n",
    "- **Time**: This is the time taken for the respective clustering initialization method to complete.\n",
    "- **Inertia**: The sum of squared distances between data points and their cluster's center. A lower inertia indicates better clustering.\n",
    "\n",
    "These three metrics evaluate the quality of clustering in terms of how well the clusters capture the true classes in your dataset. They are measures of purity and completeness.\n",
    "\n",
    "- **Homogeneity**: measures how much each cluster contains only data points that are members of a single class. Higher values are better.\n",
    "- **Completeness**: measures how well all data points that are members of a given class are assigned to the same cluster. Higher values are better.\n",
    "- **V-Measure**: the harmonic mean of homogeneity and completeness. It provides a balanced view of both measures.\n",
    "\n",
    "Finally, **Silhouette Score** measure the quality of clusters in a dataset. It takes into account both the *cohesion* (how close the data points are to each other within the same cluster) and the *separation* (how far apart different clusters are from each other). The Silhouette Score is computed for each data point and then averaged to obtain an overall score for the entire dataset. The score ranges from -1 to 1.\n",
    "\n",
    "1. High Positive Scores: If most of the Silhouette Scores are close to +1, it suggests that the clusters are well-separated and the data points are appropriately assigned to their respective clusters. This indicates a good clustering solution.\n",
    "2. Scores Around 0: If the Silhouette Scores are around 0, it indicates that data points might be on or near the decision boundary between clusters. This could imply that the clusters are overlapping or that the clustering algorithm is having difficulty distinguishing between certain data points.\n",
    "3. Negative Scores: If a significant number of data points have negative Silhouette Scores, it implies that these data points might have been assigned to the wrong clusters."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "The first step in Natural Language Processing is to get the words into a format that we can do math on them.\n",
    "\n",
    "## Pre-reading\n",
    "\n",
    "- [*Deep Learning with Python*, 11.0 - 11.3](https://learning.oreilly.com/library/view/deep-learning-with/9781617296864/Text/11.htm)\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Gain a basic understanding of natural language processing (NLP)\n",
    "- Prepare text data for computer processing.\n",
    "- Vectorize text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "> Deep learning models, being differentiable functions, can only process numeric tensors: they canâ€™t take raw text as input. *Vectorizing* text is the process of transforming text into numeric tensors.\n",
    "> ~ *Deep Learning with Python*\n",
    "\n",
    "1. **Standardize** text to make it easier to process, such as by converting it to lowercase or removing formatting.\n",
    "2. **Tokenize** the text by splitting it into units.\n",
    "3. **Index** the tokens into a numerical vector.\n",
    "\n",
    "![From raw text to vectors, *Deep Learning with Python, 2nd Ed*, fig. 11.1](../img/deep_learning_with_python-fig-11-01.png)\n",
    "\n",
    "### Standardization\n",
    "\n",
    "Examples of standardization include converting to lowercase, standardizing punctuation and special characters, and stemming.\n",
    "\n",
    "```{mermaid}\n",
    "graph LR\n",
    "   A[\"My altitude is 7258' above sea-level, far, far above that of West Point or Annapolis!\"] --> norm((\"Normalize Text\"))\n",
    "   B[\"My altitude is 7258 ft. above sea level, FAR FAR above that of west point or Annapolis!\"] --> norm\n",
    "   norm --> result[\"my altitude is 7258 feet above sea level far far above that of west point or annapolis !\"]\n",
    "```\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "You can tokenize in different ways.\n",
    "\n",
    "Here is an example of **word-level** tokenization.\n",
    "\n",
    "```json\n",
    "{\"my\", \"altitude\", \"is\", \"7258\", \"feet\", \"above\", \"sea\", \"level\", \"far\", \"far\", \"above\", \"that\", \"of\", \"west\", \"point\", \"or\", \"annapolis\", \"!\"}\n",
    "```\n",
    "\n",
    "Here is an example of **bag-of-3-grams** tokenization.\n",
    "\n",
    "```json\n",
    "{\"my altitude is\", \"altitude is 7258\", \"is 7258 feet\", \"7258 feet above\", \"feet above sea\", \"above sea level\", \"sea level far\", \"level far far\", \"far far above\", \"far above that\", \"above that of\", \"that of west\", \"of west point\", \"west point or\", \"point or annapolis\"}\n",
    "```\n",
    "\n",
    "### Indexing\n",
    "\n",
    "The simplest way to represent tokens in a vector is with the **bag-of-words** approach, which just counts how many times each token appears in the text.\n",
    "\n",
    "```json\n",
    "{'my': 1, 'altitude': 1, 'is': 1, '7258': 1, 'feet': 1, 'above': 2, 'sea': 1, 'level': 1, 'far': 2, 'that': 1, 'of': 1, 'west': 1, 'point': 1, 'or': 1, 'annapolis': 1, '!': 1}\n",
    "```\n",
    "\n",
    "As simple as this is, it can be highly effective! However, you lose sequence information, which can be critical. Moving to N-grams can help!\n",
    "\n",
    "**Sequence models** are a more advanced method of retaining sequence information, for more advanced use-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "For this exercise we will use [Inaugural Addresses from American Presidents](https://www.presidency.ucsb.edu/documents/app-categories/spoken-addresses-and-remarks/presidential/inaugural-addresses).\n",
    "\n",
    "Go to the website now and think how you might put all of these into an easy-to-ingest document.\n",
    "\n",
    "Fortunately, I've already extracted some of these and placed them in a CSV located in this folder on GitHub.\n",
    "\n",
    "### Explore Data\n",
    "\n",
    "As always, we should preview some stats about what we are diving in to.\n",
    "\n",
    "> **Prompt GPT4-Advanced Data Analytics**: Use pandas to provide a quick summary of this CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to 'inaugural_addresses.csv'\n",
    "csv_path = \"inaugural_addresses.csv\"\n",
    "\n",
    "# Load the CSV into a pandas DataFrame\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame and its summary\n",
    "df_head = df.head()\n",
    "df_info = df.info()\n",
    "\n",
    "df_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Clouds\n",
    "\n",
    "Unlike numerical data, we cannot easily do things like mean, median, or standard deviation with text data.\n",
    "\n",
    "Let's try a word cloud, just for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Set up the figure size and number of subplots\n",
    "fig, axes = plt.subplots(nrows=df.shape[0], ncols=1, figsize=(15, 30))\n",
    "\n",
    "# Loop through each row of the DataFrame and generate a word cloud\n",
    "for i, (index, row) in enumerate(df.iterrows()):\n",
    "    # Create a word cloud object\n",
    "    wc = WordCloud(\n",
    "        background_color=\"white\", stopwords=[], max_words=100, width=800, height=400\n",
    "    )\n",
    "\n",
    "    # Generate the word cloud from the 'Text' column\n",
    "    wc.generate(row[\"Text\"])\n",
    "\n",
    "    # Display the word cloud on the subplot\n",
    "    axes[i].imshow(wc, interpolation=\"bilinear\")\n",
    "    axes[i].axis(\"off\")\n",
    "    axes[i].set_title(f\"{row['President']} ({row['Year']})\", fontsize=37)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Words\n",
    "\n",
    "Hmm, that isn't very helpful! Fortunately, there are multiple lists of English [Stop Words](https://www.geeksforgeeks.org/removing-stop-words-nltk-python/) in Python.\n",
    "\n",
    "In fact, `wordcloud.STOPWORDS` is an option!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Re-produce word clouds with wordcloud.STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "Another common text pre-processing technique is [lemmatization](https://en.wikipedia.org/wiki/Lemmatization).\n",
    "\n",
    "> In linguistics, is the process of grouping together the inflected forms of a word so they can be analyzed as a single item, identified by the word's lemma, or dictionary form.\n",
    "\n",
    "**Stemming** reduces an inflected word to its base; for example: runs; running; ran --> \"run\".\n",
    "\n",
    "**Lemmatizing** goes further by using knowledge of surrounding words.\n",
    "\n",
    "1. The word \"better\" has \"good\" as its lemma. This link is missed by stemming, as it requires a dictionary look-up.\n",
    "2. The word \"walk\" is the base form for the word \"walking\", and hence this is matched in both stemming and lemmatization.\n",
    "3. The word \"meeting\" can be either the base form of a noun or a form of a verb (\"to meet\") depending on the context; e.g., \"in our last meeting\" or \"We are meeting again tomorrow\". Unlike stemming, lemmatization attempts to select the correct lemma depending on the context.\n",
    "\n",
    "\n",
    "##### Optional Exercise\n",
    "\n",
    "Re-create the inaugural address word clouds after lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization\n",
    "\n",
    "```{important}\n",
    "Delete and restart your kernel to clear out the previous runs.\n",
    "```\n",
    "\n",
    "Let's use Kagggle's [Twitter US Airline Sentiment dataset](https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment/).\n",
    "\n",
    "First, **read the Data Card**. What month and year are these from? How were they collected? What transformations have been done?\n",
    "\n",
    "### Import and explore data\n",
    "\n",
    "Yes, always the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "Let's apply stopwords and stemming to our tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the resources from nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "# Define a function that applies stemming and stopwords removal\n",
    "def preprocess(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and apply stemming\n",
    "    tokens = [\n",
    "        stemmer.stem(word)\n",
    "        for word in tokens\n",
    "        if word.lower() not in stopwords.words(\"english\")\n",
    "    ]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# Apply the function to the \"text\" column\n",
    "df[\"processed_text\"] = df[\"text\"].apply(preprocess)\n",
    "\n",
    "# Preview the result\n",
    "print(df[\"text\"].head())\n",
    "print(df[\"processed_text\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "The naive - but sometimes highly effective - approach is the \"Bag of Words\" approach.\n",
    "\n",
    "Simply count how many times words show up!\n",
    "\n",
    "Start with the train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X = df[\"processed_text\"]\n",
    "y = df[\"airline_sentiment\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer\n",
    "\n",
    "[Scikit-Learn CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "> Convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "See [Medium: Basics of CountVectorizer](https://towardsdatascience.com/basics-of-countvectorizer-e26677900f9c)\n",
    "\n",
    "![sample count vector sparse matrix](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZJZgYom-FcI1sXkp3XgR1w.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a pipeline that first transforms the text data into a bag-of-words representation\n",
    "# and then trains a logistic regression classifier\n",
    "pipeline = make_pipeline(CountVectorizer(), LogisticRegression(max_iter=1000))\n",
    "\n",
    "# Train the classifier\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}



<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>8. Linear Algebra and GPUs &#8212; ECE 495</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'dnn/gpu';</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9. Transfer Learning" href="cat-dog/transfer_learning.html" />
    <link rel="prev" title="7. Neural Networks" href="neuralnet.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    AI Hardware Applications
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Prediction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../prediction/hello-colab.html">1. Hello, Colab!</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prediction/prediction-machines.html">2. Prediction Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prediction/kmeans.html">3. K-Means clustering digits</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prediction/ml-workflow.html">4. Machine Learning Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prediction/dimensionality.html">5. Dimensionality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prediction/cc-prediction.html">6. C&amp;C: Prediction and Dimensionality</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Neural Networks</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="neuralnet.html">7. Neural Networks</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">8. Linear Algebra and GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="cat-dog/transfer_learning.html">9. Transfer Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="cat-dog/tflite_catdog.html">10. TensorFlow Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="cat-dog/live-cat.html">11. Live Inference üê± üê∂</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/usafa-ece/ai-hardware" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/usafa-ece/ai-hardware/issues/new?title=Issue%20on%20page%20%2Fdnn/gpu.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/dnn/gpu.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Linear Algebra and GPUs</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-reading">8.1. Pre-Reading</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#objectives">8.1.1. Objectives</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-algebra">8.2. Linear Algebra</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vectors-and-matrices">8.2.1. Vectors and Matrices</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dot-products">8.2.1.1. Dot Products</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-multiplication">8.2.1.2. Matrix Multiplication</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks">8.2.2. Neural Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tensorflow">8.2.2.1. TensorFlow</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graphics-processing-units">8.3. Graphics Processing Units</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-execution">8.3.1. GPU Execution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#general-matrix-multiplication">8.3.1.1. General Matrix Multiplication</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nvidia-gpus">8.3.2. NVIDIA GPUs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cuda-cores">8.3.2.1. CUDA Cores</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-cores">8.3.2.2. Tensor Cores</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#performance">8.3.3. Performance</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#math-vs-memory-bandwidth">8.3.3.1. Math vs. Memory Bandwidth</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dnn-operation-performance">8.3.3.2. DNN Operation Performance</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">8.4. Conclusion</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="linear-algebra-and-gpus">
<h1><span class="section-number">8. </span>Linear Algebra and GPUs<a class="headerlink" href="#linear-algebra-and-gpus" title="Permalink to this heading">#</a></h1>
<section id="pre-reading">
<h2><span class="section-number">8.1. </span>Pre-Reading<a class="headerlink" href="#pre-reading" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://www.techspot.com/article/2049-what-are-tensor-cores/">Explainer: What Are Tensor Cores? | TechSpot</a></p></li>
</ul>
<section id="objectives">
<h3><span class="section-number">8.1.1. </span>Objectives<a class="headerlink" href="#objectives" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Discuss why linear algebra and GPUs are the bedrock of machine learning.</p></li>
<li><p>Explain how GPU hardware accelerates mathematic computations.</p></li>
<li><p>Describe the difference between memory- and math-limited algorithms.</p></li>
</ul>
</section>
</section>
<section id="linear-algebra">
<h2><span class="section-number">8.2. </span>Linear Algebra<a class="headerlink" href="#linear-algebra" title="Permalink to this heading">#</a></h2>
<p>With NVIDIA recently becoming the <a class="reference external" href="https://www.marketwatch.com/story/nvidia-officially-closes-in-1-trillion-territory-becoming-seventh-u-s-company-to-hit-market-cap-milestone-88ead8f9">seventh company ever to hit $1 trillion market-cap</a> and the boom of AI, it is clear that GPUs are a critical hardware. But what are they and what do they actually do for AI computation?</p>
<p><strong>Answer:</strong> The vast majority of modern machine learning is handled with linear algebra and a little bit of calculus.</p>
<section id="vectors-and-matrices">
<h3><span class="section-number">8.2.1. </span>Vectors and Matrices<a class="headerlink" href="#vectors-and-matrices" title="Permalink to this heading">#</a></h3>
<blockquote>
<div><p><a class="reference external" href="https://www.3blue1brown.com/lessons/vectors">3Blue1Brown - Vectors, what even are they?</a></p>
</div></blockquote>
<ul class="simple">
<li><p>In physics, and related engineering, a vector is thought of as an arrow pointing in space; it has magnitude and direction.</p></li>
<li><p>In computer science, a vector - or array - is an ordered list, where each element corresponds with an attribute.</p></li>
<li><p>Mathematicians generalize this to say that vectors can be multiplied by each other and added to a scalar.</p></li>
</ul>
<p>A matrix is similar to a vector, but has two dimensions always expressed as <strong>rows</strong> then <strong>columns</strong>. A 2x3 matrix has two rows and three columns.</p>
<section id="dot-products">
<h4><span class="section-number">8.2.1.1. </span>Dot Products<a class="headerlink" href="#dot-products" title="Permalink to this heading">#</a></h4>
<p>A dot product is essentially how well two sets of numbers align with each other. A dot product with a small magnitude means little alignment, while a large magnitude means more alignment.</p>
<p>When you open a door you push near the knob because that gives you good alignment between the force on the door and the motion of the door. Mathematically, this is a large dot product.
If you try to open a door by pushing near the hinge then there is poor alignment between the force and motion. This is a small dot product, and it is very difficult to get the door to swing!</p>
<p>The dot product can be calculated as either the product of the magnitudes (lengths) of each vector and the cosine of the angel between them. Visually, this is the transformation of two arrows in space.</p>
<div class="math notranslate nohighlight">
\[a \cdot b = |a|\cdot|b|\cdot\cos(\theta)\]</div>
<p>Alternatively, the dot product can be calculated as the multiplication and summation of each element of two matrices. This formation will be more helpful for understanding the application to machine learning.</p>
<div class="math notranslate nohighlight">
\[\mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^{n} a_i b_i\]</div>
</section>
<section id="matrix-multiplication">
<h4><span class="section-number">8.2.1.2. </span>Matrix Multiplication<a class="headerlink" href="#matrix-multiplication" title="Permalink to this heading">#</a></h4>
<p>Two matrices can be multiplied together. The multiplication of an <span class="math notranslate nohighlight">\(m\times n\)</span> matrix with an <span class="math notranslate nohighlight">\(n \times p\)</span> matrix will be an <span class="math notranslate nohighlight">\(m \times p\)</span> matrix. Notice that the inner, <span class="math notranslate nohighlight">\(n\)</span>, dimensions must match in order for a dot product to be possible.</p>
</section>
</section>
<section id="neural-networks">
<h3><span class="section-number">8.2.2. </span>Neural Networks<a class="headerlink" href="#neural-networks" title="Permalink to this heading">#</a></h3>
<p>A neural network is a system of interconnected neurons inspired by the function of the human brain. The neurons exist in three types of layers:</p>
<ol class="arabic simple">
<li><p><strong>Input layer</strong> takes the sample as a vector</p></li>
<li><p><strong>Hidden layers</strong> handle the work of extracting features from the sample</p></li>
<li><p><strong>Output layer</strong> provides probabilities that a sample belongs to a particular class</p></li>
</ol>
<p>The neurons in each layer are linked through a series of connections, each of which is assigned a specific weight. The weight of a link from one neuron to the next signifies the importance of the first neuron to the neuron in the next layer.</p>
<p>When conducting inference, the input layer will take in values. To determine the value of each neuron in the next layer, the network multiplies each input value by the corresponding weight and then adds up these products. This is known as the <em>weighted sum</em>. If the incoming values are represented as a vector <span class="math notranslate nohighlight">\(V\)</span> and the connection weights are represented as a vector <span class="math notranslate nohighlight">\(W\)</span> then the weighted sum at a neuron <span class="math notranslate nohighlight">\(N\)</span> is the dot product of the two vectors. In addition to the weights associated with the connections, each neuron in the network possesses an additional parameter known as a <em>bias</em>, <span class="math notranslate nohighlight">\(B\)</span>. The bias allows for adjusting the output of the neuron along with the weighted sum. Thus, the final value of a neuron is:
$<span class="math notranslate nohighlight">\(N = (\vec{V} \cdot \vec{W}) + B\)</span>$
Once the total of the weighted sum and the bias crosses a particular threshold, the neuron becomes <em>activated</em>. There are numerous activation functions; two common ones are rectified linear unit (<a class="reference external" href="https://builtin.com/machine-learning/relu-activation-function">ReLU</a>) and sigmoid.</p>
<p>Overall, the interaction between weights, biases, and activation functions allows neural networks to learn complex patterns and make informed predictions.</p>
<section id="tensorflow">
<h4><span class="section-number">8.2.2.1. </span>TensorFlow<a class="headerlink" href="#tensorflow" title="Permalink to this heading">#</a></h4>
<p>In mathematics, a <em>tensor</em> is an algebraic object that describes multilinear relationships between sets of other algebraic objects related to a vector space.</p>
<p><a class="reference external" href="https://www.tensorflow.org/about">TensorFlow</a> is an open source end-to-end machine learning platform developed by Google. It constrains the definition of tensors in <a class="reference external" href="https://www.tensorflow.org/guide/tensor">Introduction to Tensors</a>: <strong>tensors are multi-dimensional arrays with a uniform type.</strong></p>
<p>TensorFlow uses tensors to construct neural networks.</p>
<p>Matricies and vectors are subsets of tensors, so matrix multiplication and dot products are common operations on tensors as well. TensorFlow makes extensive use of elementwise multiplication and summation during both training and inference.</p>
</section>
</section>
</section>
<section id="graphics-processing-units">
<h2><span class="section-number">8.3. </span>Graphics Processing Units<a class="headerlink" href="#graphics-processing-units" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p><a class="reference external" href="https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html">GPU Performance Background User‚Äôs Guide - NVIDIA Docs</a></p>
</div></blockquote>
<p>NVIDIA GPUs consist of a number of Streaming Multiprocessors (SM), on-chip L2 cache, and high-bandwidth DRAM. Arithmetic and other instructions are executed by the SMs; data and code are accessed from DRAM via the L2 cache. Each SM has its own instruction schedulers and various instruction execution pipelines.</p>
<p><img alt="Simplified view of the GPU architecture" src="https://docscontent.nvidia.com/dita/00000186-1a08-d34f-a596-3f291b140000/deeplearning/performance/dl-performance-gpu-background/graphics/simple-gpu-arch.svg" /></p>
<p><strong>Multiply-add is the most frequent operation in modern neural networks</strong>, acting as a building block for fully-connected and convolutional layers, both of which <strong>can be viewed as a collection of vector dot-products</strong>.</p>
<section id="gpu-execution">
<h3><span class="section-number">8.3.1. </span>GPU Execution<a class="headerlink" href="#gpu-execution" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>GPUs execute many threads concurrently.</p></li>
<li><p>GPUs execute functions using a 2-level hierarchy of threads. A given function‚Äôs threads are grouped into equally-sized¬†<em>thread blocks</em>, and a set of thread blocks are launched to execute the function.</p></li>
<li><p>GPUs hide dependent instruction latency by switching to the execution of other threads. Thus, the number of threads needed to effectively utilize a GPU is much higher than the number of cores or instruction pipelines.</p></li>
<li><p>At runtime, a block of threads is placed on an SM for execution, enabling all threads in a thread block to communicate and synchronize efficiently.</p></li>
</ul>
<p><img alt="Utilization of an 8-SM GPU when 12 thread blocks with an occupancy of 1 block/SM at a time are launched for execution. Here, the thread blocks execute in 2 waves, the first wave utilizes 100% of the GPU, while the 2nd wave utilizes only 50%." src="https://docscontent.nvidia.com/dita/00000186-1a08-d34f-a596-3f291b140000/deeplearning/performance/dl-performance-gpu-background/graphics/utilize-8sm-gpu.svg" /></p>
<section id="general-matrix-multiplication">
<h4><span class="section-number">8.3.1.1. </span>General Matrix Multiplication<a class="headerlink" href="#general-matrix-multiplication" title="Permalink to this heading">#</a></h4>
<p>GPUs implement general matrix multiplication (<a class="reference external" href="https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#gpu-imple">GEMM</a>) by partitioning the output matrix into tiles, which are then assigned to thread blocks.</p>
<p>Each thread block computes its output tile by stepping through the K dimension in tiles, loading the required values from the A and B matrices, and multiplying and accumulating them into the output C matrix.
<img alt="Tiled outer product approach to GEMMs" src="https://docscontent.nvidia.com/dita/00000186-1a08-d34f-a596-3f291b140000/deeplearning/performance/dl-performance-matrix-multiplication/graphics/tiled-outer-prod.svg" /></p>
</section>
</section>
<section id="nvidia-gpus">
<h3><span class="section-number">8.3.2. </span>NVIDIA GPUs<a class="headerlink" href="#nvidia-gpus" title="Permalink to this heading">#</a></h3>
<p>US based <a class="reference external" href="https://www.nvidia.com/en-us/about-nvidia/">NVIDIA</a> is the world leader in GPU design. They are fabless, meaning they design chips but do not make them themselves. Currently Taiwan Semiconductor Manufacturing Co. (TSMC) <a class="reference external" href="https://www.reuters.com/business/nvidia-ceo-says-interested-exploring-chip-manufacturing-with-intel-2022-03-23/">makes the bulk of NVIDIA chips</a>.</p>
<p>Two critical NVIDIA technologies are CUDA Cores and Tensor Cores</p>
<section id="cuda-cores">
<h4><span class="section-number">8.3.2.1. </span>CUDA Cores<a class="headerlink" href="#cuda-cores" title="Permalink to this heading">#</a></h4>
<p><a class="reference external" href="https://developer.nvidia.com/blog/even-easier-introduction-cuda/">An Even Easier Introduction to CUDA | NVIDIA Technical Blog</a></p>
<p><strong>CUDA</strong> is an API that allows softare to directly access NVIDIA GPU instruction set. The CUDA Toolkit allows developers to use C++ to interact with the cores, but many libraries - such as TensorFlow - have implmented CUDA under the hood.</p>
<p>CUDA is used in numerous applications, from scientific computing to gaming.</p>
</section>
<section id="tensor-cores">
<h4><span class="section-number">8.3.2.2. </span>Tensor Cores<a class="headerlink" href="#tensor-cores" title="Permalink to this heading">#</a></h4>
<p>Tensor Cores were introduced in the NVIDIA Volta‚Ñ¢ GPU architecture to accelerate matrix multiply-add operations for machine learning and scientific applications.</p>
<ul class="simple">
<li><p>These instructions operate on small matrix blocks (for example, 4x4 blocks).</p></li>
<li><p>These smaller matrix blocks are then aggregated.</p></li>
<li><p>Tensor Cores can compute and accumulate products in higher precision than the inputs. For example, during training with FP16 inputs, Tensor Cores can compute products without loss of precision and accumulate in FP32.</p></li>
<li><p>When math operations cannot be formulated in terms of matrix blocks - for example, element-wise addition - they are executed in CUDA cores.</p></li>
<li><p>Effeciency is best when matrix dimensions are multiples of 16 bytes.</p></li>
</ul>
<p>Tenosr Cores continue to be improved with additional supported data types in the Turing Architecture.</p>
</section>
</section>
<section id="performance">
<h3><span class="section-number">8.3.3. </span>Performance<a class="headerlink" href="#performance" title="Permalink to this heading">#</a></h3>
<p>Performance of a function on a given processor is limited by one of the following three factors; <strong>memory bandwidth, math bandwidth and latency</strong>.</p>
<ul class="simple">
<li><p>In cases of insuffecient parallelism, latency will be the greatest limiting factor.</p></li>
<li><p>If there is suffecient parallelism, math or memory will be the greatest limiting factor, based on specific arithmetic intensity of the algorithm and the math vs. memory bandwidth of the processor.</p></li>
</ul>
<section id="math-vs-memory-bandwidth">
<h4><span class="section-number">8.3.3.1. </span>Math vs. Memory Bandwidth<a class="headerlink" href="#math-vs-memory-bandwidth" title="Permalink to this heading">#</a></h4>
<p>How much time is spent in memory or math operations depends on both the algorithm and its implementation, as well as the processor‚Äôs bandwidths.</p>
<ul class="simple">
<li><p><strong>arithmetic intensity</strong> is the ratio of the number of mathematical operations vs. the number of bytes accessed.</p></li>
<li><p>the <strong>ops:byte ratio</strong> is the ratio of math bandwidth vs. memory bandwidth for a given processor.</p></li>
<li><p>an algorithim is <strong>math limited</strong> on a given processor if the arithmetic intesity is higher than  the processor‚Äôs ops:byte ratio.</p></li>
<li><p>an algorithim is <strong>memory limited</strong> on  a given processor if the arithmetic intensity is lower than the processor‚Äôs ops:byte ratio.</p></li>
</ul>
</section>
<section id="dnn-operation-performance">
<h4><span class="section-number">8.3.3.2. </span>DNN Operation Performance<a class="headerlink" href="#dnn-operation-performance" title="Permalink to this heading">#</a></h4>
<p>Modern deep neural networks are built from a variety of layers, who‚Äôs operations fall into three categories.</p>
<ol class="arabic simple">
<li><p><strong>Elementwise operations</strong> are independent of all other elements within the tensor. Examples include summation or ReLU. These tend to be <em>memory-limited.</em></p></li>
<li><p><strong>Reduction operations</strong> produce values computed over a range of input tensor values, to include pooling, batch normalization, or computing means. These tend to be <em>memory- limited.</em></p></li>
<li><p><strong>Dot-Product Operations</strong> typically involve a weight tensor and an activation tensor. In the case of fully-connected layers, these dot products are typically expressed as matrix-matrix. These may be <em>math-limited if the matricies are large enough,</em> otherwise they are memory-limited.</p></li>
</ol>
</section>
</section>
</section>
<section id="conclusion">
<h2><span class="section-number">8.4. </span>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">#</a></h2>
<p>The execution of linear algebra on GPUs is foundational to modern machine learning. Linear algebra provides the mathematical framework that underpins many machine learning algorithms. Neural networks, in particular, are heavily reliant on using weighted sums dot products to propagate information and adjust weights through the network layers.</p>
<p>Graphics Processing Units (GPUs) with their highly parallelized structures have revolutionized the computational efficiency of these operations. Technologies like NVIDIA‚Äôs CUDA and Tensor Cores allow for the optimized execution of the matrix and vector operations that are so common in machine learning, significantly accelerating the training and inference processes of deep learning models.</p>
<p>Understanding the performance dynamics of these GPUs‚Äîsuch as memory bandwidth, math bandwidth, and latency‚Äîallows for the optimized implementation of algorithms. Deep neural networks often need to be carefully designed and tuned to effectively leverage the computational capabilities of GPUs.</p>
<p>In essence, the interplay of linear algebra and GPU technology forms the bedrock of contemporary machine learning, enabling the creation and operation of complex models that can learn from and make predictions on vast amounts of data.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./dnn"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="neuralnet.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">7. </span>Neural Networks</p>
      </div>
    </a>
    <a class="right-next"
       href="cat-dog/transfer_learning.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">9. </span>Transfer Learning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-reading">8.1. Pre-Reading</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#objectives">8.1.1. Objectives</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-algebra">8.2. Linear Algebra</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vectors-and-matrices">8.2.1. Vectors and Matrices</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dot-products">8.2.1.1. Dot Products</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-multiplication">8.2.1.2. Matrix Multiplication</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks">8.2.2. Neural Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tensorflow">8.2.2.1. TensorFlow</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graphics-processing-units">8.3. Graphics Processing Units</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-execution">8.3.1. GPU Execution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#general-matrix-multiplication">8.3.1.1. General Matrix Multiplication</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nvidia-gpus">8.3.2. NVIDIA GPUs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cuda-cores">8.3.2.1. CUDA Cores</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-cores">8.3.2.2. Tensor Cores</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#performance">8.3.3. Performance</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#math-vs-memory-bandwidth">8.3.3.1. Math vs. Memory Bandwidth</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dnn-operation-performance">8.3.3.2. DNN Operation Performance</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">8.4. Conclusion</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By DFEC
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      ¬© Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>